\documentclass{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}


\geometry{
a4paper,
right=20mm,
left=20mm,
top=20mm,
bottom=20mm,	
}

\begin{document}

\pagenumbering{gobble}

\begin{center}
\textbf{\huge Homework 1 : CS772} \\
\vspace{5pt}
\textit{\Large Jayant Agrawal}         14282
\end{center}

\section*{Problem 1}
\textbf{MLE} finds the parameter $\theta$ that maximises the log-likelihood($p(X|\theta)$)
$$\mathcal{L}(\theta) = log p(X|\theta) = log p(x_1, x_2 ...x_N | \theta)$$
Since the observations are i.i.d,
$$ p(x_1, x_2 ...x_n | \theta) = \prod_{n=1}^N p(x_n| \theta)$$
MLE Estimation,
$$\theta_{MLE} = argmax_{\theta} \mathcal{L}(\theta) = argmax_{\theta} \sum_{n=1}^N logp(x_n|\theta)$$
For poisson distribution,
$$\lambda_{MLE} = argmax_{\lambda} \sum_{n=1}^N logp(x_n|\lambda)$$
$$\lambda_{MLE} = argmax_{\lambda} \sum_{n=1}^N log\frac{\lambda^{x_n}e^{-\lambda}}{x_n!}$$
$$\lambda_{MLE} = argmax_{\lambda} \hspace{5pt} log\lambda\sum_{n=1}^Nx_n - N\lambda - \sum_{n=1}^Nlog(x_n!)$$
Setting the derivative to 0 to find $\lambda_{MLE}$,
$$\frac{1}{\lambda}\sum_{n=1}^Nx_n - N = 0$$
$$\mathbf{\lambda_{MLE}} = \frac{1}{N}\sum_{n=1}^Nx_n$$
\textbf{MAP} finds the parameter $\theta$ that maximises the log posterior probability($p(\theta|X)$)
$$\mathcal{L}(\theta) = log p(\theta|X) = log\frac{p(X|\theta)p(\theta)}{p(X)} $$
Since the observations are i.i.d, MAP estimation($\theta_{MAP}$) is given by,
$$\theta_{MAP} = argmax_{\theta} \mathcal{L}(\theta) = argmax_{\theta}\sum_{n=1}^N\log{p(x_n|\theta)}+ \log{p(\theta)} $$
For poisson distribution,
$$\lambda_{MAP} = argmax_{\lambda} \log{\lambda}\sum_{n=1}^Nx_n- N\lambda - \sum_{n=1}^Nlog(x_n!) + \log{\frac{\beta^\alpha}{\tau(\alpha)} \lambda^{\alpha-1} e^{-\beta\lambda}}$$
Setting the derivative to 0 to find $\lambda_{MAP}$,
$$\frac{1}{\lambda}\sum_{n=1}^Nx_n - N + \frac{\alpha-1}{\lambda}- \beta= 0$$
$$\mathbf{\lambda_{MAP}} = \frac{\sum_{n=1}^Nx_n + \alpha-1}{N+\beta}$$
\textbf{Posterior} Distribution will be proportional to the product of likelihood and prior:
$$ p(\theta| X) \propto \prod_{n=1}^N p(x_n|\theta) p(\theta) $$
For poisson distribution,
$$p(\lambda|X) \propto \prod_{n=1}^N p(x_n|\lambda) p(\lambda)$$
$$p(\lambda|X) \propto \prod_{n=1}^N \frac{\lambda^{x_n}e^{-\lambda}}{x_n!} \frac{\beta^\alpha}{\tau(\alpha)} \lambda^{\alpha-1}e^{-\beta \lambda}$$
$$p(\lambda|X) \propto  \frac{1}{\prod_{n=1}^{N}x_n!} \lambda^{\alpha + \sum_{n=1}^Nx_n -1}e^{-\lambda(N+\beta)}$$
Note that this is also a Gamma Distribution as(the proportionality constant comes after integrating the marginal likelihood in the denominator in the original expression of the posterior over $\lambda$), 
$$p(\lambda|X) = \text{Gamma}(\alpha + \sum_{n=1}^Nx_n, \beta + N)$$
Clearly, MAP estimate can be written as weighted combination of the MLE estimate and the prior's mode as,
$$\frac{\sum_{n=1}^Nx_n + \alpha-1}{N+\beta} = a*\frac{1}{N}\sum_{n=1}^Nx_n + b*\frac{\alpha-1}{\beta}$$
where $a$ and $b$ are scalars whose values are,
$$a = \frac{N}{N+\beta}$$
$$b = \frac{\beta}{N+\beta}$$
Since, posterior is also a Gamma Distribution, its mean is given by $\frac{\alpha_{pos}}{\beta_{pos}}$, which is,
$$\text{Posterior's Mean} = \frac{\sum_{n=1}^{N}x_n+ \alpha}{N+\beta}$$ 
Thus, Posterior's mean can be written as a weighted combination of the MLE estimate and the prior's mean as,
$$\frac{\sum_{n=1}^Nx_n + \alpha}{N+\beta} = a*\frac{1}{N}\sum_{n=1}^Nx_n + b*\frac{\alpha}{\beta}$$
where $a$ and $b$ are scalars whose values are,
$$a = \frac{N}{N+\beta}$$
$$b = \frac{\beta}{N+\beta}$$
\section*{Problem 2}
The variance of the posterior predictive distribution \textbf{decreases}. Intuitively, this is because, as the number of training examples increases, the uncertainty in the model decreases as we now we have some extra information about the underlying distribution. Formally, this can be seen by analysing the difference between $\sigma^2_{N+1}(x_*)$ and $\sigma^2_{N}(x_*)$.
$$\sigma^2_{N+1}(x_*) - \sigma^2_{N}(x_*) = (\beta^{-1} + x_*^{T}\Sigma_{N+1}x_*) - (\beta^{-1} + x_*^{T}\Sigma_{N}x_*)$$
$$\sigma^2_{N+1}(x_*) - \sigma^2_{N}(x_*) = x_*^{T}(\Sigma_{N+1} - \Sigma_N)x_* $$
Let $\Sigma_N/\beta = M_N^{-1}$ 
$$\sigma^2_{N+1}(x_*) - \sigma^2_{N}(x_*) = \beta*x_*^{T}((M_{N}+x_{N+1}x_{N+1}^T)^{-1} - M_N^{-1})x_* $$
Since we only care about the sign of this,
$$\text{sgn}(\sigma^2_{N+1}(x_*) - \sigma^2_{N}(x_*)) = \text{sgn}((M_{N}+x_{N+1}x_{N+1}^T)^{-1} - M_N^{-1}) $$
Using the identity given in the problem statement, 
$$\text{sgn}(\sigma^2_{N+1}(x_*) - \sigma^2_{N}(x_*)) = \text{sgn}(-\frac{(M_N^{-1}x_{N+1})(x_{N+1}^{T} M_N^{-1})}{1+x_{N+1}^TM_N^{-1}x_{N+1}}) $$
Since $M_N = (\sum_{n=1}^{N}x_nx_{n+1}^T + \frac{\lambda}{\beta
}I)^{-1}$, therefore, 
$$\text{sgn}(\sigma^2_{N+1}(x_*) - \sigma^2_{N}(x_*)) = -1 $$
This shows that $\sigma^2_{N+1}(x_*)$ is less than $\sigma^2_{N}(x_*)$, thus proving that the variance decreases on increasing the number of training examples.

\section*{Problem 3}


\end{document}


