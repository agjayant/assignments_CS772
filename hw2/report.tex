\documentclass{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{dsfont}

\geometry{
a4paper,
right=20mm,
left=20mm,
top=20mm,
bottom=20mm,	
}

\begin{document}

\pagenumbering{gobble}

\begin{center}
\textbf{\huge CS772 : Probabilistic Machine Learning} \\
\textbf{\huge Homework 2} \\
\vspace{5pt}
\textit{\Large Jayant Agrawal} \\
14282
\end{center}

\section*{Problem 1}
\textbf{Constructing Richer Priors \\ \\}
Posterior $p(\theta|D)$ is given by:
$$p(\theta|D) = \frac{p(D|\theta)p(\theta)}{p(D)}$$
The prior is given as:
$$p(\theta) = \sum_{k=1}^Kp(z=k)p(\theta|z=k)$$
Since $p(\theta|z=k)$ is conjugate to $p(D|\theta)$ for all $k$. We can write posterior as:
$$p(\theta|D) = p(D|\theta)\sum_{k=1}^Kp(z=k)p(\theta|z=k)/p(D)$$
$$ = \sum_{k=1}^K p(z=k)\frac{p(D|\theta)p(\theta|z=k)}{p(D)}$$
Thus, posterior can be written as a convex combination of conjugate distributions. Also we can further write posterior as:
$$ = \sum_{k=1}^K p(z=k)p(\theta|D, z=k)$$
This can be interpreted as a convex combination of $k$ posteriors. Also, this is of the same form as the prior. Thus, we can say that $p(\theta)$ is a conjugate to the likelihood $p(D|\theta)$.
\section*{Problem 2}
\textbf{Regression using Generative Models} \\ \\
An appropriate choice for $p(x,y)$ is the \textbf{bivariate gaussian distribution}.
$$p(x,y) = \mathcal{N}(\hspace{2pt} (x,y)\hspace{2pt} | \hspace{2pt} (\mu_x,\mu_y), \Sigma)$$
where,
\[
\Sigma=
  \begin{bmatrix}
    \Sigma_{xx} & \Sigma_{xy}  \\
    \Sigma_{yx} & \Sigma_{yy} 
  \end{bmatrix}
\]
The parameters to be learned here are: $\mu_x$, $\mu_y$ and $\Sigma$. \\ \\
Bivariate Gaussian is a reasonable choice since marginals and conditionals of a multivariate gaussian are also gaussian, as we will see next. And it makes even more sense, as the discriminative linear regression model was also based on a gaussian distribution. \\ \\
Computing the actual distribution of interest, $p(y|x)$ is easy since, conditional distribution of a bivariate gaussian is another gaussian as,
$$p(y|x) = \mathcal{N}(y|\mu_{y|x}, \Sigma_{y|x})$$
$$\mu_{y|x} = \mu_y + \Sigma_{yx}\Sigma_{xx}^{-1}(x-\mu_x)$$
$$\Sigma_{y|x} = \Sigma_{yy} - \Sigma_{yx}\Sigma_{xx}^{-1}\Sigma_{xy}$$
The parameters to be learned here are: $\mu_x$, $\mu_y$ and $\Sigma$.
The log likelihood of the model is given by:
$$\mathcal{L} = \prod_{n=1}^N\mathcal{N}(\hspace{2pt} (x_n,y_n)\hspace{2pt} | \hspace{2pt} (\mu_x,\mu_y), \Sigma)$$
Since this is a bivariate gaussian, we can intuitively see that $\mu_x^{MLE}$ and $\mu_y^{MLE}$ will be:
$$\mu_x^{MLE} = \frac{1}{N}\sum_{n=1}^Nx_n$$
$$\mu_y^{MLE} = \frac{1}{N}\sum_{n=1}^Ny_n$$
And as we know, the solution for $\Sigma$ will be similar to what we have seen in class (skipping computations):
$$\Sigma = \frac{1}{N}\sum_{n=1}^N \sum_{n=1}^N (\tilde{x_n}-\tilde{\mu})(\tilde{x_n}-\tilde{\mu})^T$$
where $\tilde{x_n} = (x_n,y_n)$ and $\tilde{\mu} = (\mu_x, \mu_y)$.
\section*{Problem 3}
The complete data likelihood is given by:
$$p(x,z) = \prod_{n=1}^Np(x_n,z_n|\Theta)$$
where,
$$p(x_n, z_n|\Theta) = p(x_n|z_n)p(z_n)$$	
$$p(x_n, z_n|\Theta) = \mathcal{N}(x_n|\mu_{z_n}, \Sigma_{z_n})\pi_{z_n}$$	
Thus, 
$$p(x,z) = \prod_{n=1}^N\pi_{z_n}\mathcal{N}(x_n|\mu_{z_n}, \Sigma_{z_n})$$
$$p(x,z) = ( \prod_{n=1}^N\pi_{z_n})(\prod_{n=1}^{N}\mathcal{N}(x_n|\mu_{z_n}, \Sigma_{z_n}))$$
Since the observations are i.i.d, the product of gaussians is another gaussian, 
$$p(x,z) = f(\Theta) \times \text{Gaussian}$$
$$p(x,z) = h(x) \times exp(\theta^T \phi(x) - A(\theta))$$
This is thus of the form of an exponential family distribution.
\section*{Problem 4}
\textbf{Mixture Model for Binary Vectors} \\ \\
\emph{Bernoulli} Destribution is a distribution over r.v. $x\in\{0,1\}$. Thus, for a binary vector of size $D$, product of Bernoulli is appropriate choice where each feature $x_{nd}$ in every observation $x_n$ is sampled from a Bernoulli. Also, let the cluster/component ID for observation $x_n$ be $z_n$, which is sampled from a \emph{multinoulli} distribution. If $\pi=(\pi_1, \pi_2,..., \pi_k)$, where $\sum_{k=1}^K \pi_k = 1$, then
$$p(z_n = k | \pi) = \pi_k$$
Also, given $z_n$, we generate $x_n$ from $k$-th cluster as,
$$p(x_n|z_n=k) = \prod_{d=1}^D \text{Bernoulli}(x_{nd}, \rho_{kd})$$
Complete data likelihood ($p(x_n,z_n|\Theta)$) for $x_n$ is,
$$p(x_n,z_n|\Theta) = p(x_n|z_n, \rho)p(z_n, \pi)$$
where $\Theta=\{\{\pi\}_{k=1}^K, \{\rho_{dk}\}_{d=1,k=1}^{D,K}\}$ For MAP Estimate, we have to consider $\prod_{n=1}^Np(x_n,z_n|\Theta)p(\Theta)$. Thus, the complete data log-likelihood becomes,
$$\text{CLL}(\Theta) = \log(\prod_{n=1}^Np(x_n,z_n|\Theta)p(\Theta) )$$
Choosing \emph{Dirichlet} prior for \{$\pi$\} and \emph{Beta} prior for $\{\rho_{dk}\}$, we derive the following expression for CLL$(\Theta)$, for computing the MAP Estimate,
$$\text{CLL}(\Theta) = \sum_{n=1}^N \log(\prod_{k=1}^K [p(x_n|z_n=k)p(z_n=k)]^{z_{nk}} ) + \log{p(\pi)} + \log{p(\rho)} $$
where $z_{nk}$ is one if $x_n$ belongs to the $k$-th cluster, otherwise 0,
$$\text{CLL}(\Theta) = \sum_{n=1}^N \sum_{k=1}^K z_{nk}[\log{\pi_k}+ \log{\prod_{d=1}^D\text{Bernoulli}(x_{nd}, \rho_{kd})}] + \log{\text{Dirichlet}(\pi; \delta)} + \sum_{k=1}^K \sum_{d=1}^D \log{ \text{Beta}(\rho_{kd}; \alpha, \beta) }$$
where $\{\delta\}$ and $\{\alpha,\beta\}$ are hyperparameters for Dirichlet and Beta Distributions respectively. \\ \\
\textbf{E Step} \\
Here, we assume $\Theta$ is known, and compute expectation for $z_{nk}$, as
\begin{equation*}
\begin{split}
\mathds{E}[z_{nk}] &= p(z_{nk} = 1| x_n)  \\
 &\propto p(z_{nk} = 1)p(x_n|z_{nk} = 1) \\
 &= \pi_k \prod_{d=1}^D (\rho_{kd})^{x_{nd}}(1-\rho_{kd})^{1-x_{nd}}  
 \end{split}
\end{equation*}
Normalizing, we get $\mathds{E}[z_{nk}] = \gamma_{nk}$, 
$$\gamma_{nk} = \frac{\pi_k \prod_{d=1}^D (\rho_{kd})^{x_{nd}}(1-\rho_{kd})^{1-x_{nd}}}{\sum_{l=1}^K\pi_l \prod_{d=1}^D (\rho_{ld})^{x_{nd}}(1-\rho_{ld})^{1-x_{nd}}}$$
\textbf{M Step} \\
Using $\mathds{E}[z_{nk}] = \gamma_{nk}$, computed in \emph{E Step}, maximising expected CLL objective wrt $\Theta = \{\pi,\rho\}$ will give us the MAP Estimate,
$$\mathcal{L} = \sum_{n=1}^N \sum_{k=1}^K \gamma_{nk}[\log{\pi_k}+ \log{\prod_{d=1}^D\text{Bernoulli}(x_{nd}, \rho_{kd})}] + \log{\text{Dirichlet}(\pi; \delta)} + \sum_{k=1}^K \sum_{d=1}^D \log{ \text{Beta}(\rho_{kd}; \alpha, \beta) }$$

$$\mathcal{L} = \sum_{n=1}^N \sum_{k=1}^K \gamma_{nk}[\log{\pi_k}+ \log{\prod_{d=1}^D(\rho_{kd})^{x_{nd}}(1-\rho_{kd})^{1-x_{nd}}}] + \log{A \times \prod_{k=1}^K\pi_k^{\delta_k-1}} + \sum_{k=1}^K \sum_{d=1}^D \log{B \times \rho_{kd}^{\alpha-1}(1-\rho_{kd})^{\beta-1} }$$
where $A$ and $B$ are some constants. To compute MAP Estimate for $\rho_{kd}$,
$$\frac{\partial \mathcal{L}}{\partial \rho_{kd}} = 0$$
Ignoring all the parts which are not a function of $\rho_{dk}$, we get,
$$\frac{\partial \mathcal{L}}{\partial \rho_{kd}} = \sum_{n=1}^N\gamma_{nk}(\frac{x_{nd}}{\rho_{kd}} - \frac{1-x_{nd}}{1-\rho_{kd}}) + \frac{\alpha-1}{\rho_{kd}} - \frac{\beta-1}{1-\rho_{kd}} = 0$$
$$\rho_{kd} = \frac{\sum_{n=1}^N\gamma_{nk}x_{nd}+\alpha-1}{N_k+\alpha+\beta-2}$$
where $N_k = \sum_{n=1}^N\gamma_{nk}$. To compute MAP Estimate for $\pi_k$, we have to use lagrange multiplier, $\lambda$ to incorporate the constraint $\sum_{k=1}^K\pi_k=1$. Ignoring all the parts which do not depend on $\pi_k$, we get the following objective,
$$\mathcal{L}' = \sum_{n=1}^N \sum_{k=1}^K \gamma_{nk}\log{\pi_k} + \sum_{k=1}^K(\delta_k-1)\log{\pi_k} + \lambda(1-\sum_{k=1}^K\pi_k)$$
For $\pi_k$,
$$\frac{\partial \mathcal{L}'}{\partial \pi_{k}} = 0$$
$$\sum_{n=1}^N\frac{\gamma_{nk}}{\pi_k}+ \frac{\delta_k-1}{\pi_k} -\lambda = 0$$
$$\pi_k = \frac{\sum_{n=1}^N\gamma_{nk}+\delta_k-1}{\lambda}$$
For $\lambda$,
$$\frac{\partial \mathcal{L}'}{\partial \lambda} = 0$$
$$\frac{1}{\lambda}\sum_{n=1}^N\sum_{k=1}^K \gamma_{nk} + \frac{1}{\lambda}\sum_{k=1}^K(\delta_k-1)= 1$$
$$\lambda=N - K + \sum_{k=1}^K\delta_k$$
Thus, 
$$\pi_k = \frac{\sum_{n=1}^N\gamma_{nk}+\delta_k-1}{N - K + \sum_{k=1}^K\delta_k}$$
\section*{Problem 5}
\textbf{EM for Hyperparameter Estimation} \\ \\
Complete Data Likelihood, $p(y,w|X,\lambda, \beta) $, is given by :
$$p(y,w|X,\lambda, \beta) = p(y|w,X,\beta)p(w|\lambda)$$
$$p(y,w|X,\lambda, \beta) = \prod_{n=1}^N \mathcal{N}(y_n|w^Tx_n, \beta^{-1})\mathcal{N}(w|0,\lambda^{-1}I)$$
Complete Data Log-Likelihood is given by :
$$\log p(y,w|X,\lambda, \beta) = \sum_{n=1}^N \log \mathcal{N}(y_n|w^Tx_n, \beta^{-1})+\log \mathcal{N}(w|0,\lambda^{-1}I)$$
$$\log p(y,w|X,\lambda, \beta) = \frac{N}{2}\log \beta - \frac{\beta}{2}\sum_{n=1}^N(y_n-w^Tx_n)^2 - \frac{1}{2}\log |\lambda^{-1}I| -\frac{1}{2}\lambda(w^Tw)$$
Now, since we are using EM and taking expectation wrt w, we have to compute $\mathds{E}[w^T]$ and $\mathds{E}[w^Tw]$, to be able to do MLE for $\lambda$ and $\beta$ on the expected complete data log likelihood. Thus, the EM Algorithm is as follows: \\ \\
\textbf{E Step} \\ \\
In this step, posterior of $w$ needs to be computed to compute the two expectations above. Computing posterior:
$$p(w|X,y,\beta,\lambda) \propto p(w|\lambda)p(y|X,w,\beta)$$
Using Gaussian properties, we get: (this is exactly similar as done in class, so skipping the computations)
$$p(w|X,y,\beta,\lambda) = \mathcal{N}(\mu, \Sigma)$$
$$\mu = (X^TX + \frac{\lambda}{\beta}I_D)^{-1}X^Ty$$ 
$$\Sigma = (\beta X^TX + \lambda I_D)^{-1}$$
Therefore, required expectations are given by:
$$\mathds{E}[w^T] = \mathds{E}[w]^T =\mu^T$$
$$\mathds{E}[w^Tw] = \mathds{E}[w]^T\mathds{E}[w]+\text{trace}(cov(w)) = \mu^T\mu + trace(\Sigma)$$
\textbf{M Step} \\ \\
Expected Complete Data log likelihood is given by:
$$\mathcal{L} =  \mathds{E}[\log p(y,w|X,\lambda, \beta)] = \frac{N}{2}\log \beta - \frac{\beta}{2}\sum_{n=1}^N(y_n-\mathds{E}[w^T]x_n)^2 - \frac{1}{2}\log |\lambda^{-1}I| -\frac{1}{2}\lambda(\mathds{E}[w^Tw])$$
For $\beta$, 
$$\frac{\partial \mathcal{L}}{\partial \beta} = 0$$
$$\frac{N}{2\beta} - \frac{1}{2}\sum_{n=1}^N(y_n-\mathds{E}[w^T]x_n)^2 = 0$$
$$\beta = \frac{N}{\sum_{n=1}^N(y_n-\mu^Tx_n)^2}$$
For $\lambda$,
$$\frac{\partial \mathcal{L}}{\partial \lambda} = 0$$
$$\frac{D}{2\lambda} - \frac{\mathds{E}[w^Tw]}{2} = 0$$
$$\lambda = \frac{D}{ \mu^T\mu + trace(\Sigma)}$$
\end{document}


